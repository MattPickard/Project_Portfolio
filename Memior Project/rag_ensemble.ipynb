{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A RAG Ensemble Pipeline Implementation**  \n",
    "\n",
    "**Overview**  \n",
    "This is an ensemble of the following RAG (Retrieval-Augmented Generation) techniques:\n",
    "- Query Rewriting\n",
    "- LLM-based Reranker (previously used cross-encoder reranker in comments)\n",
    "- Context Retrieval\n",
    "\n",
    "This pipeline uses:\n",
    "- LangChain\n",
    "- FAISS (Facebook AI Similarity Search)\n",
    "- OpenAI embeddings\n",
    "- GPT-4o-mini API\n",
    "\n",
    "**Implementation Reference**  \n",
    "[https://github.com/NirDiamant/RAG_Techniques](https://github.com/NirDiamant/RAG_Techniques)\n",
    "\n",
    "**Preprocessing**  \n",
    "I preprocessed my grandfather's memoir titled \"My Life Story\" into 10 PDFs (chapters). Each PDF was processed using Fitz into continuous strings and chunked into langchain Document objects. Metadata was added to each chunk to aid in retrieval of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from .env file that contains the OpenAI API key\n",
    "load_dotenv() \n",
    "\n",
    "# Get OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the PDF paths\n",
    "paths = [os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\", file) for file in os.listdir(os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdfs(paths, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Preprocesses PDFs using Fitz then encodes chunks into a vector store using OpenAI \n",
    "    embeddings while saving source and index as metadata. \n",
    "        paths: A list of paths to the PDF files.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded content of the PDFs with citations.\n",
    "    \"\"\"\n",
    "\n",
    "    all_texts = []\n",
    "\n",
    "    index = 0  # Initialize a global index to keep track of the index across all chunks\n",
    "\n",
    "    for path in paths:\n",
    "        # Open the PDF document located at the specified path\n",
    "        doc = fitz.open(path)\n",
    "        content = \"\"\n",
    "        # Iterate over each page in the document\n",
    "        for page_num in range(len(doc)):\n",
    "            # Get the current page\n",
    "            page = doc[page_num]\n",
    "            # Extract the text content from the current page and append it to the content string\n",
    "            content += page.get_text()\n",
    "        # Divide the content into chunks of specified size with overlap.\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(content):\n",
    "            end = start + chunk_size\n",
    "            chunk = content[start:end]\n",
    "            # Chunk is concatinated \n",
    "            chunks.append(Document(page_content=chunk))\n",
    "            # The start position is incremented by the chunk size minus the overlap to ensure consecutive chunks overlap.\n",
    "            start += chunk_size - chunk_overlap\n",
    "        # Extract file name from path\n",
    "        file_name = os.path.basename(path)\n",
    "        \n",
    "        # Update metadata instead of appending to page_content\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata.update({\n",
    "                \"index\": index,\n",
    "                \"source\": file_name\n",
    "            })\n",
    "            index += 1  # Increment the global index for each chunk\n",
    "\n",
    "        all_texts.extend(chunks)\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(all_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "chunk_overlap= 200\n",
    "# Encode the PDFs\n",
    "chunks_vector_store = encode_pdfs(paths, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vector store\n",
    "#chunks_vector_store.save_local(\"my_life_story_ensemble.json\")\n",
    "\n",
    "#load the vector store\n",
    "chunks_vector_store = FAISS.load_local(\"my_life_story_ensemble.json\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_by_index(vectorstore, target_index):\n",
    "    \"\"\"\n",
    "    Retrieve a chunk from the vectorstore based on its index in the metadata. Will be called in\n",
    "    get_contex().\n",
    "    \n",
    "    Args:\n",
    "    vectorstore (VectorStore): The vectorstore containing the chunks.\n",
    "    target_index (int): The index of the chunk to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    Optional[Document]: The retrieved chunk as a Document object, or None if not found.\n",
    "    \"\"\"\n",
    "    # Retrieve all documents from the vectorstore\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "    \n",
    "    # Search for the document with the specified index\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get('index') == target_index:\n",
    "            return doc\n",
    "            \n",
    "    # If not found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query):\n",
    "    \"\"\"\n",
    "    Rewrites the original query to improve retrieval.\n",
    "    \n",
    "    Args:\n",
    "    original_query (str): The original user query\n",
    "    \n",
    "    Returns:\n",
    "    str: The rewritten query\n",
    "    \"\"\"\n",
    "    re_write_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "\n",
    "    # Create a prompt template for query rewriting\n",
    "    query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "    The following query is a question pertaining to George Shambaugh's life. Reword the same question in 3 very concise ways, using \n",
    "    examples of first-person as if George is asking himself and third-person as if someone else is asking about him.\n",
    "\n",
    "    Original query: {original_query}\n",
    "\n",
    "    Rewritten query:\"\"\"\n",
    "\n",
    "    query_rewrite_prompt = PromptTemplate(\n",
    "        input_variables=[\"original_query\"],\n",
    "        template=query_rewrite_template\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain for query rewriting\n",
    "    query_rewriter = query_rewrite_prompt | re_write_llm\n",
    "    \n",
    "    response = query_rewriter.invoke(original_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM-based Reranker\n",
    "class RatingScore(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a rating score for a document's relevance to a query.\n",
    "    \n",
    "    Attributes:\n",
    "    relevance_score (float): The relevance score of a document to a query.\n",
    "    \"\"\"\n",
    "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")\n",
    "\n",
    "def reranker(new_query, chunks_vector_store, top_n: int = 3):\n",
    "    \"\"\"\n",
    "    Reranks documents based on their relevance to a new query using an LLM model.\n",
    "    \n",
    "    Args:\n",
    "    new_query (str): The new query to search for relevant documents.\n",
    "    chunks_vector_store: The vector store to query.\n",
    "    top_n (int, optional): The number of top-ranked documents to return. Defaults to 3.\n",
    "    \n",
    "    Returns:\n",
    "    List[Document]: A list of documents reranked by their relevance to the new query.\n",
    "    \"\"\"\n",
    "    # Retrieve initial documents based on the new query\n",
    "    docs = chunks_vector_store.similarity_search(new_query, k=10)\n",
    "\n",
    "    # Define a prompt template for the LLM to rate document relevance\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"doc\"],\n",
    "        template= \"\"\"On a scale of 1-10, rate the relevance of the following chunk from \n",
    "        George Shambaugh's memoir to the query. Consider the specific context and intent \n",
    "        of the query, not just keyword matches.\n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the LLM model for rating document relevance\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
    "    \n",
    "    # Score each document based on its relevance to the new query\n",
    "    scored_docs = []\n",
    "    for doc in docs:\n",
    "        input_data = {\"query\": new_query, \"doc\": doc.page_content}\n",
    "        score = llm_chain.invoke(input_data).relevance_score\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0  # Default score if parsing fails\n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    # Sort documents by their relevance scores in descending order\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    # Return the top N reranked documents\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-encoder Reranker (not used in lastest enseble)\n",
    "\"\"\"def reranker(new_query, chunks_vector_store, rerank_top_k = 3):\n",
    "    Retrieve and rerank documents based on the query using a cross-encoder model.\n",
    "\n",
    "    #Args:\n",
    "    #query (str): The query to search for relevant documents.\n",
    "    #chunks_vector_store: The vector store to query.\n",
    "\n",
    "    #Returns:\n",
    "    #List[str]: A list of documents reranked by their relevance to the query\n",
    "\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # Initial retrieval\n",
    "    initial_docs = chunks_vector_store.similarity_search(new_query, k=10)\n",
    "\n",
    "    # Prepare pairs for cross-encoder\n",
    "    pairs = [[new_query, doc.page_content] for doc in initial_docs]\n",
    "\n",
    "    # Get cross-encoder scores\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    # Sort documents by score and include index metadata\n",
    "    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return top reranked documents with their index metadata\n",
    "    return [doc for doc, _ in scored_docs[:rerank_top_k]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(chunks_vector_store, reranked_chunks, num_neighbors: int = 4, chunk_overlap: int = 200):\n",
    "    \"\"\"\n",
    "    This function retrieves the context (surrounding chunks) of the reranked chunks \n",
    "    and concatonates them together accounting for overlap.\n",
    "\n",
    "    Args:\n",
    "        chunks_vector_store: The vector store to query.\n",
    "        reranked_chunks: The reranked chunks to retrieve the context for.\n",
    "        num_neighbors: The number of neighboring chunks to retrieve.\n",
    "        chunk_overlap: The amount of overlap between neighboring chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of context sequences for the reranked chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    result_sequences = []\n",
    "\n",
    "    for chunk in reranked_chunks:\n",
    "        current_index = chunk.metadata.get('index', None)\n",
    "        if current_index is None:\n",
    "            continue\n",
    "\n",
    "        # Determine the range of chunks to retrieve\n",
    "        start_index = max(0, current_index - num_neighbors)\n",
    "        end_index = current_index + num_neighbors + 1  # +1 because range is exclusive at the end\n",
    "\n",
    "        # Retrieve all chunks in the range\n",
    "        neighbor_chunks = []\n",
    "        for i in range(start_index, end_index):\n",
    "            neighbor_chunk = get_chunk_by_index(chunks_vector_store, i)\n",
    "            if neighbor_chunk:\n",
    "                neighbor_chunks.append(neighbor_chunk)\n",
    "\n",
    "        # Check if neighbor_chunks is empty\n",
    "        if not neighbor_chunks:\n",
    "            continue  # Skip to the next chunk if no neighbors found\n",
    "\n",
    "        # Concatenate chunks, accounting for overlap\n",
    "        concatenated_text = neighbor_chunks[0].page_content\n",
    "        for i in range(1, len(neighbor_chunks)):\n",
    "            current_chunk = neighbor_chunks[i].page_content\n",
    "            overlap_start = max(0, len(concatenated_text) - chunk_overlap)\n",
    "            concatenated_text = concatenated_text[:overlap_start] + current_chunk\n",
    "\n",
    "        result_sequences.append(concatenated_text)\n",
    "\n",
    "    return result_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    \"\"\"\n",
    "    Model to generate an answer to a query based on a given context.\n",
    "    \n",
    "    Attributes:\n",
    "        answer_based_on_content (str): The generated answer and citation based on the context.\n",
    "    \"\"\"\n",
    "    answer_based_on_content: str = Field(description=\"Generates an answer and [citation] to a query based on a given context.\")\n",
    "    \n",
    "def create_question_answer_from_context_chain(llm):\n",
    "    # Initialize the ChatOpenAI model with specific parameters\n",
    "    question_answer_from_context_llm = llm\n",
    "\n",
    "    # Define the prompt template for chain-of-thought reasoning\n",
    "    question_answer_prompt_template = \"\"\" \n",
    "    You are querying a memior called \"My Life Story\" written by George Shambaugh.\n",
    "    For the question below, provide a concise but suffice answer. If you don't know, only write \"The RAG retrieval was unable to provide sufficient context\":\n",
    "    {context}\n",
    "    Question\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PromptTemplate object with the specified template and input variables\n",
    "    question_answer_from_context_prompt = PromptTemplate(\n",
    "        template=question_answer_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Create a chain by combining the prompt template and the language model\n",
    "    question_answer_from_context_cot_chain = question_answer_from_context_prompt | question_answer_from_context_llm.with_structured_output(\n",
    "        QuestionAnswerFromContext)\n",
    "    return question_answer_from_context_cot_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_context(question, context, question_answer_from_context_chain):\n",
    "    \"\"\"\n",
    "    Answer a question using the given context by invoking a chain of reasoning.\n",
    "\n",
    "    Args:\n",
    "        question: The question to be answered.\n",
    "        context: The context to be used for answering the question.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the answer, context, and question.\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "    output = question_answer_from_context_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    return {\"answer\": answer, \"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args:\n",
    "        context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i + 1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RAG(original_query):\n",
    "    \"\"\"\n",
    "    Test the Retrieval-Augmented Generation (RAG) process with a given query. It also prints the context chunks retrieved from the vector store.\n",
    "\n",
    "    Args:\n",
    "        original_query (str): The query to be tested against the vector store created from my Grandfather's memoir.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the language model based on the retrieved context.\n",
    "    \"\"\"\n",
    "    # Rewrite the original query to enhance its retrieval capabilities\n",
    "    new_query = rewrite_query(original_query)\n",
    "    # Rerank chunks from the vector store based on the enhanced query\n",
    "    reranked_chunks = reranker(new_query, chunks_vector_store)\n",
    "    # Extract context from the reranked chunks\n",
    "    context = get_context(chunks_vector_store, reranked_chunks)\n",
    "    # Initialize the language model with specific parameters\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=2000)\n",
    "    # Create a chain for question answering from context\n",
    "    question_answer_from_context_chain = create_question_answer_from_context_chain(llm)\n",
    "    # Answer the question using the context and the chain\n",
    "    answer = answer_question_from_context(original_query, context, question_answer_from_context_chain)\n",
    "    # Print the original query, enhanced query, and the response\n",
    "    print(\"Original Query:\", original_query + \"\\n\" + \"Enhanced Query:\", new_query + \"\\n\" )\n",
    "    print(\"Response:\", answer[\"answer\"], \"\\n\")\n",
    "    # Display the context chunks\n",
    "    show_context(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What is an example of one of his mom's sayings, and what did he refer to these sayings as?\n",
      "Enhanced Query: 1. What’s an example of my mom's sayings, and what do I call them?  \n",
      "2. Can you provide an example of George's mom's sayings and what he refers to them as?  \n",
      "3. What did he call his mom's sayings, and can you give an example?  \n",
      "\n",
      "Response: One example of one of his mom's sayings is, \"I told you not to climb that tree! When you fall out and break both your legs, don’t come running into me!\" He referred to these sayings as \"Momisms.\" \n",
      "\n",
      "Context 1:\n",
      "ter if I were late. I tried to be careful carrying out \n",
      "the full pan of water, but accidents happen even to the best of us, not to mention us Klutzes. \n",
      " \n",
      "Early in life at Republic Avenue we boys were playing cowboys and cattle rustlers. Since I was oldest, I \n",
      "was the Head Cowboy. Since Chub was next oldest, he was the Cattle Rustler. Paul and Bob were part \n",
      "of my posse. After yelling and chasing Chub until we caught him, we decided to hang him – that’s what \n",
      "cowboys do to dirty, low-down cattle rustlers. We took Chub into the garage. It was unfinished and had \n",
      "overhead exposed beams. We found an old, huge rope, about an inch in diameter. We put an old, 55-\n",
      "gallon oil drum upside down under one of the rafters and had Chub stand on the drum. I threw the \n",
      "rope over the rafter and tied it around under Chub’s arms. I then kicked the drum from under him, \n",
      "waiting to see him dangle from the rafter. He didn’t dangle. The rotten old rope Dad had discarded \n",
      "broke under the slightest weight and Chub fell to the garage floor (no big deal) and hit the back of his \n",
      "head on the oil drum (big deal). It cut his head open slightly, enough for Mom the put an end to the \n",
      "posse. Justice was thwarted again by a totalitarian regime. \n",
      " \n",
      "There were other Momisms. We boys were forbidden to climb the tree in the front yard, so of course \n",
      "we would. Mom would say, ”I told you not to climb that tree! When you fall out and break both your \n",
      "legs, don’t come running into me!” We wouldn’t. If Mom was told that Paul hit Bob on the head with \n",
      "the ball bat, she would say “It’s a good thing it was his head or he could have been hurt.”. Substitute \n",
      "any boy’s name for either person., Mom had other sayings: “I know where you are going if you don’t \n",
      "mend your ways”; “If you spill salt it is bad luck unless yu throw a pinch of it over your left shoulder.” \n",
      "Don’t look cross-eyed are they will get stuck that way!” ”You only go to the hospital to die.” One saying \n",
      "Dad was heard to say was “He is so dumb, he couldn’t pour piss out of a boot with directions on the \n",
      "toe and a spigot on the heel”. We knew how dumb that was. Tim remembers that Dad S. would call \n",
      "baked beans “crackers.” \n",
      " \n",
      "I never was motivated to play sports. In the eighth grade Mr. Tippy (school Principal) said the whole \n",
      "class had to try out for Field Events – racing, jumping and the like. One of Mom’s favorite description of \n",
      "us boys was that we “ran like a bunch of striped-assed apes.” I guess she was right – as usual. I outran \n",
      "all the other taller boys in school. After winning other races in the area, I was part of a team to \n",
      "represent us to the State Meet at the OSU football stadium. I didn’t win anything against those big guys \n",
      "with good training, but I had fun running up and down the bleacher stairs. \n",
      " \n",
      "Since we lived out of the city, we planted a garden and slowly accumulated a menagerie of animals. \n",
      "We always had a dog, \n",
      "\n",
      "\n",
      "Context 2:\n",
      " thought it was time to talk again.” We began visiting the Thompson farm near \n",
      "Circleville, OH. On Saturday evening, if we had time to stay, the family would go into Circleville to the \n",
      "street fair and see cartoons they showed for everyone. It was the first we kids had seen cartoons, since \n",
      "we never had been to a movie. TV was a laboratory process then. \n",
      " \n",
      "Another time during the laundry, Mom heard someone at the front door again. I answered the found a \n",
      "man who asked, “Is Jennie at home?” I said, “Yes, wait a minute” and closed the door again. Displeased \n",
      "at the interruption, Mom answered the door and almost fainted. She said, “Frank! I thought you were \n",
      "dead!” It was her brother Uncle Frank whom she had not seen in 16 years. After both of Mom’s \n",
      "parents died, she had been placed in Big Sisters Home and Uncle Frank went into the Army during \n",
      "World War I. After Uncle Frank had come back from Europe, married, divorced, then roamed East \n",
      "coast, he came to find Mom. He stayed in the Columbus area until World War II, when re reenlisted in \n",
      "the Army for World War II (Fig 37). He returned to Columbus after the war was over. It wasn’t until \n",
      "about ten years later that Aunt Ann Cartzdafner, Mom’s half-sister, contacted our family again, having \n",
      "lived on the south side of Columbus all those years. \n",
      " \n",
      "During those years Dad frequently worked two shifts at Golco Service Station. If Mom had to go to an \n",
      "appointment or to the grocery store, Dad asked Pat (George) Evans, a young black man he knew from \n",
      "the station, to drive Mom where she had to go. We enjoyed Pat and trusted him. One day I \n",
      "accompanied Pat to go uptown for an errand for Mom. He stopped and bought each of us a cupcake. I \n",
      "was ecstatic, since I had never had a cupcake before. Pat joined the Army during World War II. He sent \n",
      "a card and a cake home to Mom, his “Mother”, while he was still in the Army. Mom kept the card. He \n",
      "maintained contact with Mom and Dad over all the years. The last I saw him was when he attended \n",
      "Dad’s funeral.  \n",
      " \n",
      "I would like to comment on the uniforms that Dad wore while working at Golco. At first, he wore what \n",
      "looked to me like horse-riding pants with black leather leggings (Fig 38). I remember polishing Dad’s \n",
      "leggings for his outfit. I found only one photo of Dad in that outfit when he was talking to Uncle Ben. \n",
      "Later he and his colleague Lester White wore regular long pants at work. \n",
      "EAST LINDEN - REVISITED \n",
      "In 1939 we moved again to East Linden, this time to 2047 Republic Avenue, one block from Columbus \n",
      "city limits, but almost a mile from East Linden Elementary School. I was in fifth grade with the same \n",
      "students with whom I had started first grade. Since students left and new students arrived every year, \n",
      "no one recognized me as a former classmate. Tootie and Ann were both in the eighth grade that first \n",
      "year. Then they graduated and went to Mifflin High School abo\n",
      "\n",
      "\n",
      "Context 3:\n",
      "hey brought from school. I took books from the school library to read. When the \n",
      "teacher asked me what book I would report on, I told her Gulliver’s Travels. She thought Mom had \n",
      "read the book to me, but I said Mom didn’t have time to do that. When I gave my report, I mangled all \n",
      "of the strange words like Lilliputians and Yahoos, but convinced the teacher I had read it myself. The \n",
      "result of that was that I was immediately sent to Third Grade, missing all of the important skills in \n",
      "arithmetic second grade gives you. I caught up with the class fairly soon. \n",
      " \n",
      "Bob was born there in 1936 (Fig 36). Chub and Dorothy started school at Fulton Street Elementary in \n",
      "the same class. By that time Dorothy walked poorly, but there were no special classes for handicapped \n",
      "students. Dorothy did well in school scholastically. Our house was narrow and long with two bedrooms \n",
      "and a bathroom upstairs and a living room and kitchen downstairs. We had electricity and indoor \n",
      "plumbing, but we had no refrigerator or washing machine. Mom did the washing on a wash board in a \n",
      "large tub of sudsy water, I would turn the crank on the wringer to put the clothes into another tub of \n",
      "clear water for rinsing, then wring them again to be hung on a line. Laundry was a long, arduous task. \n",
      " \n",
      "Two incidents occurred during those years that I remember. During laundry, Mom heard someone at \n",
      "the front door. I answered the door to find a woman there who said, “Is Jennie home?” I said, “Yes, just \n",
      "a minute,” and closed the door. In this part of Columbus, one would not invite in a stranger or leave \n",
      "the door open. Mom was not happy about stopping work and drying off to see anyone.  Mom opened \n",
      "the door and said, “Bird! What are you doing here?” I had never seen Aunt Bertha to remember her. \n",
      "Aunt Bertha said, “I thought it was time to talk again.” We began visiting the Thompson farm near \n",
      "Circleville, OH. On Saturday evening, if we had time to stay, the family would go into Circleville to the \n",
      "street fair and see cartoons they showed for everyone. It was the first we kids had seen cartoons, since \n",
      "we never had been to a movie. TV was a laboratory process then. \n",
      " \n",
      "Another time during the laundry, Mom heard someone at the front door again. I answered the found a \n",
      "man who asked, “Is Jennie at home?” I said, “Yes, wait a minute” and closed the door again. Displeased \n",
      "at the interruption, Mom answered the door and almost fainted. She said, “Frank! I thought you were \n",
      "dead!” It was her brother Uncle Frank whom she had not seen in 16 years. After both of Mom’s \n",
      "parents died, she had been placed in Big Sisters Home and Uncle Frank went into the Army during \n",
      "World War I. After Uncle Frank had come back from Europe, married, divorced, then roamed East \n",
      "coast, he came to find Mom. He stayed in the Columbus area until World War II, when re reenlisted in \n",
      "the Army for World War II (Fig 37). He returned \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_RAG(\"What is an example of one of his mom's sayings, and what did he refer to these sayings as?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
