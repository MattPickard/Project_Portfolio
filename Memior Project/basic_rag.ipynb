{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Basic RAG Pipeline Implementation**  \n",
    "\n",
    "**Overview**  \n",
    "This is a basic RAG (Retrieval-Augmented Generation) pipeline implementation using:\n",
    "- PyPDFLoader and RecursiveCharacterTextSplitter\n",
    "- LangChain\n",
    "- FAISS (Facebook AI Similarity Search)\n",
    "- OpenAI embeddings\n",
    "- GPT-4o-mini API\n",
    "\n",
    "**Implementation Reference**  \n",
    "[https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb)\n",
    "\n",
    "**Preprocessing**  \n",
    "My grandfather's memoir titled \"My Life Story\" was split into 10 PDFs (chapters). Each PDF was processed using PyPDFLoader and chunked with RecursiveCharacterTextSplitter and cleaned of tab characters. A citation to the source chapter was appended to the end of each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from .env file that contains the OpenAI API key\n",
    "load_dotenv() \n",
    "\n",
    "# Get OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the PDF paths\n",
    "paths = [os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\", file) for file in os.listdir(os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdfs(paths, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Encodes multiple PDFs into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        paths: A list of paths to the PDF files.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded content of the PDF chunks with appended citations.\n",
    "    \"\"\"\n",
    "\n",
    "    all_cleaned_texts = []\n",
    "\n",
    "    for path in paths:\n",
    "        # Load PDF documents\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "        )\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "        # Extract file name from path\n",
    "        file_name = os.path.basename(path)\n",
    "\n",
    "        # Append document citation to the end of each chunk\n",
    "        for text in cleaned_texts:\n",
    "            text.page_content = text.page_content + f\" [Source: {file_name}]\"\n",
    "\n",
    "        all_cleaned_texts.extend(cleaned_texts)\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = get_langchain_embedding_provider(EmbeddingProvider.OPENAI)\n",
    "\n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(all_cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the PDFs\n",
    "chunks_vector_store = encode_pdfs(paths, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vector store\n",
    "#chunks_vector_store.save_local(\"basic_rag_citation.json\")\n",
    "\n",
    "#load the vector store\n",
    "chunks_vector_store = FAISS.load_local(\"my_life_story_basic_rag_citation.json\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    \"\"\"\n",
    "    Model to generate an answer to a query based on a given context.\n",
    "    \n",
    "    Attributes:\n",
    "        answer_based_on_content (str): The generated answer and citation based on the context.\n",
    "    \"\"\"\n",
    "    answer_based_on_content: str = Field(description=\"Generates an answer and [citation] to a query based on a given context.\")\n",
    "    \n",
    "def create_question_answer_from_context_chain(llm):\n",
    "    # Initialize the ChatOpenAI model with specific parameters\n",
    "    question_answer_from_context_llm = llm\n",
    "\n",
    "    # Define the prompt template for chain-of-thought reasoning\n",
    "    question_answer_prompt_template = \"\"\" \n",
    "    You are querying a memior called \"My Life Story\" written by George Shambaugh.\n",
    "    For the question below, provide a concise but suffice answer. If you don't know, only write \"The RAG retrieval was unable to provide sufficient context\":\n",
    "    {context}\n",
    "    Question\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PromptTemplate object with the specified template and input variables\n",
    "    question_answer_from_context_prompt = PromptTemplate(\n",
    "        template=question_answer_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Create a chain by combining the prompt template and the language model\n",
    "    question_answer_from_context_cot_chain = question_answer_from_context_prompt | question_answer_from_context_llm.with_structured_output(\n",
    "        QuestionAnswerFromContext)\n",
    "    return question_answer_from_context_cot_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_context(question, context, question_answer_from_context_chain):\n",
    "    \"\"\"\n",
    "    Answer a question using the given context by invoking a chain of reasoning.\n",
    "\n",
    "    Args:\n",
    "        question: The question to be answered.\n",
    "        context: The context to be used for answering the question.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the answer, context, and question.\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "    output = question_answer_from_context_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    return {\"answer\": answer, \"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args:\n",
    "        context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i + 1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RAG(test_query):\n",
    "    \"\"\"\n",
    "    Test the Retrieval-Augmented Generation (RAG) process with a given query. It also prints the context chunks retrieved from the vector store.\n",
    "\n",
    "    Args:\n",
    "        test_query (str): The query to be tested against the vector store created from my Grandfather's memoir.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the language model based on the retrieved context.\n",
    "    \"\"\"\n",
    "    # Retrieve chunks related to the test query from the vector store\n",
    "    chunks = chunks_query_retriever.invoke(test_query)\n",
    "    # Extract the content of each chunks to form the context\n",
    "    context = [chunk.page_content for chunk in chunks]\n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=2000)\n",
    "    # Create a prompt template and combine with the language model\n",
    "    question_answer_from_context_chain = create_question_answer_from_context_chain(llm)\n",
    "    # Answer the question based on the retrieved context\n",
    "    answer = answer_question_from_context(test_query, context, question_answer_from_context_chain)\n",
    "    # Print the response generated by the language model\n",
    "    print(\"Response:\", answer[\"answer\"], \"\\n\")\n",
    "    # Display the context chunks retrieved from the vector store\n",
    "    show_context(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: One example of one of his mom's sayings is, \"I told you not to climb that tree! When you fall out and break both your legs, don’t come running into me!\" He referred to these sayings as \"Momisms.\" \n",
      "\n",
      "Context 1:\n",
      "There were other Momisms. We boys were forbidden to climb the tree in the front yard, so of course \n",
      "we would. Mom would say, ”I told you not to climb that tree! When you fall out and break both your \n",
      "legs, don’t come running into me!” We wouldn’t. If Mom was told that Paul hit Bob on the head with \n",
      "the ball bat, she would say “It’s a good thing it was his head or he could have bee n hurt.”. Substitute \n",
      "any boy’s name for either person., Mom had other sayings: “I know where you are going if you don’t \n",
      "mend your ways”; “If you spill salt it is bad luck unless yu throw a pinch of it over your left shoulder.” \n",
      "Don’t look cross-eyed are they will get stuck that way!” ”You only go to the hospital to die.” One saying \n",
      "Dad was heard to say was “He is so dumb, he couldn’t pour piss out of a boot with directions on the \n",
      "toe and a spigot on the heel”. We knew how dumb that was. Tim remembers that Dad S. would call \n",
      "baked beans “cra ckers.” [Source: Chapter 2.pdf]\n",
      "\n",
      "\n",
      "Context 2:\n",
      "problem. Since he stepped over the slate he had intruded on Pooch’s private domain and suffered the \n",
      "consequences.   \n",
      "We had War Ration Books for each family member containing food stamps. Mom always had more \n",
      "stamps to buy food than she had money to pay for it. I still have my books numbers 1 and 3 with \n",
      "leftover stamps in them. We had macaroni and cheese a lot. I would walk to the store and ask for a \n",
      "nickel’s worth of cheese. The butcher would cut off a chunk from the slab and wrap it for me. Dad had \n",
      "been a baker before he was married. During one meal Dad said to Mom,”The pie crust is not short \n",
      "enough.” Mom said “OK Shambaugh! From now on you do the baking around here”. He didn’t, but we \n",
      "knew when Mom called him Shambaugh, something was about to hit the fan. [Source: Chapter 2.pdf]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_RAG(\"What is an example of one of his mom's sayings, and what did he refer to these sayings as?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
