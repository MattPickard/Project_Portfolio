{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up - Define file location\n",
    "filename = \"E:\\\\Turbofan Engine Degradation Simulation Data\\\\N-CMAPSS_DS03-012.h5\"\n",
    "h5_file = h5py.File(filename, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time tracking, Operation time (min):  0.003\n",
    "t = time.process_time()  \n",
    "\n",
    "# Load data\n",
    "with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))           # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "        X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "        A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "        X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "        T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))\n",
    "                          \n",
    "W = np.concatenate((W_dev, W_test), axis=0)  \n",
    "X_s = np.concatenate((X_s_dev, X_s_test), axis=0)\n",
    "X_v = np.concatenate((X_v_dev, X_v_test), axis=0)\n",
    "T = np.concatenate((T_dev, T_test), axis=0)\n",
    "Y = np.concatenate((Y_dev, Y_test), axis=0) \n",
    "A = np.concatenate((A_dev, A_test), axis=0) \n",
    "    \n",
    "print('')\n",
    "print(\"Operation time (min): \" , (time.process_time()-t)/60)\n",
    "print('')\n",
    "print (\"W shape: \" + str(W.shape))\n",
    "print (\"X_s shape: \" + str(X_s.shape))\n",
    "print (\"X_v shape: \" + str(X_v.shape))\n",
    "print (\"T shape: \" + str(T.shape))\n",
    "print (\"A shape: \" + str(A.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert W_var and X_s_var from lists to pandas DataFrames with appropriate column names\n",
    "W_df = pd.DataFrame(W, columns=W_var)\n",
    "X_s_df = pd.DataFrame(X_s, columns=X_s_var) \n",
    "A_df = pd.DataFrame(A, columns=A_var)\n",
    "Y_df = pd.DataFrame(Y, columns=[\"RUL\"])\n",
    "\n",
    "# Concatenate project variables into one DataFrame\n",
    "df = pd.concat([W_df, X_s_df, A_df, Y_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = df[df['unit'].isin([16, 18, 20, 11, 14, 15])] #from DS02-006 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = df[df['unit'].isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])] #from DS03-012 dataset\n",
    "test_unit_13 = df[df['unit'] == 13]\n",
    "test_unit_14 = df[df['unit'] == 14]\n",
    "test_unit_15 = df[df['unit'] == 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train_df1 and train_df2\n",
    "train_df = pd.concat([train_df1, train_df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del hdf, df, W_df, X_s_df, A_df, Y_df, W_var, X_s_var, A_var, W, X_s, A, Y, train_df1, train_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove flight class and cycle column\n",
    "train_df = train_df.drop(columns=['Fc', 'cycle'])\n",
    "test_unit_13 = test_unit_13.drop(columns=['Fc', 'cycle'])\n",
    "test_unit_14 = test_unit_14.drop(columns=['Fc', 'cycle'])\n",
    "test_unit_15 = test_unit_15.drop(columns=['Fc', 'cycle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape\n",
    "print(train_df.shape)\n",
    "print(test_unit_13.shape)\n",
    "print(test_unit_14.shape)\n",
    "print(test_unit_15.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df, window_size=30, step_size=10):\n",
    "    \"\"\"\n",
    "    Groups the DataFrame into non-overlapping time windows with a specified step size.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - window_size (int): The number of rows per window.\n",
    "    - step_size (int): The number of rows to skip between windows.\n",
    "\n",
    "    Returns:\n",
    "    - List[pd.DataFrame]: A list of DataFrame windows.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for i in range(0, len(df) - window_size + 1, step_size):\n",
    "        window = df.iloc[i:i + window_size].reset_index(drop=True)\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "train_windows = create_windows(train_df)\n",
    "test_unit_13_windows = create_windows(test_unit_13)\n",
    "test_unit_14_windows = create_windows(test_unit_14)\n",
    "test_unit_15_windows = create_windows(test_unit_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del train_df, test_unit_13, test_unit_14, test_unit_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_homogeneous_windows(windows):\n",
    "    \"\"\"\n",
    "    Removes windows where all units are not the same.\n",
    "\n",
    "    Parameters:\n",
    "    - windows (List[pd.DataFrame]): A list of DataFrame windows.\n",
    "\n",
    "    Returns:\n",
    "    - List[pd.DataFrame]: A list of DataFrame windows with homogeneous units.\n",
    "    \"\"\"\n",
    "    homogeneous_windows = []\n",
    "    for window in windows:\n",
    "        if window['unit'].nunique() == 1: \n",
    "            homogeneous_windows.append(window)\n",
    "    return homogeneous_windows\n",
    "\n",
    "print(len(train_windows))\n",
    "train_windows = remove_non_homogeneous_windows(train_windows)\n",
    "print(\"After removing non-homogeneous windows: \", len(train_windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unit_column(windows):\n",
    "    \"\"\"\n",
    "    Removes the 'unit' column from each window.\n",
    "\n",
    "    Parameters:\n",
    "    - windows (List[pd.DataFrame]): A list of DataFrame windows.\n",
    "\n",
    "    Returns:\n",
    "    - List[pd.DataFrame]: A list of DataFrame windows without the 'unit' column.\n",
    "    \"\"\"\n",
    "    updated_windows = [window.drop(columns=['unit']) for window in windows]\n",
    "    return updated_windows\n",
    "\n",
    "train_windows = remove_unit_column(train_windows)\n",
    "test_unit_13_windows = remove_unit_column(test_unit_13_windows)\n",
    "test_unit_14_windows = remove_unit_column(test_unit_14_windows)\n",
    "test_unit_15_windows = remove_unit_column(test_unit_15_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_windows(windows):\n",
    "    \"\"\"\n",
    "    Randomizes the order of windows.\n",
    "\n",
    "    Parameters:\n",
    "    - windows (List[pd.DataFrame]): A list of DataFrame windows.\n",
    "\n",
    "    Returns:\n",
    "    - List[pd.DataFrame]: A list of randomized DataFrame windows.\n",
    "    \"\"\"\n",
    "    randomized_windows = windows[:]  \n",
    "    random.shuffle(randomized_windows)  \n",
    "    return randomized_windows\n",
    "\n",
    "# Randomize training windows\n",
    "train_windows = randomize_windows(train_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split trainging windows into training and validation\n",
    "train_windows, validation_windows = train_test_split(train_windows, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_x_y(windows, label_columns=['RUL', 'hs']):\n",
    "    \"\"\"\n",
    "    Separates the features (X) from the labels (y) in the windows.\n",
    "    \n",
    "    Parameters:\n",
    "    - windows (List[pd.DataFrame]): A list of DataFrame windows.\n",
    "    - label_columns (List[str]): The names of the label columns.\n",
    "    \n",
    "    Returns:\n",
    "    - X (np.ndarray): Features.\n",
    "    - y (np.ndarray): Labels with shape (num_windows, num_labels).\n",
    "    \"\"\"\n",
    "    X = np.array([window.drop(columns=label_columns).values for window in windows])\n",
    "    y = np.array([[window[label].iloc[0] for label in label_columns] for window in windows])\n",
    "    return X, y\n",
    "\n",
    "train_x, train_y = separate_x_y(train_windows)\n",
    "validation_x, validation_y = separate_x_y(validation_windows)\n",
    "test_unit_13_x, test_unit_13_y = separate_x_y(test_unit_13_windows)\n",
    "test_unit_14_x, test_unit_14_y = separate_x_y(test_unit_14_windows)\n",
    "test_unit_15_x, test_unit_15_y = separate_x_y(test_unit_15_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del train_windows, validation_windows, test_unit_13_windows, test_unit_14_windows, test_unit_15_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape and y labels\n",
    "print(\"Train x shape: \", train_x.shape)\n",
    "print(\"Train y shape: \", train_y.shape)\n",
    "print(\"Train y[0]: \", train_y[0])\n",
    "print(\"Train y[1]: \", train_y[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data\\\\train_df_30.h5', 'w') as hdf:\n",
    "    hdf.create_dataset('x', data=train_x, compression='gzip', compression_opts=9)\n",
    "    hdf.create_dataset('y', data=train_y, compression='gzip', compression_opts=9)\n",
    "with h5py.File('data\\\\validation_df_30.h5', 'w') as hdf:\n",
    "    hdf.create_dataset('x', data=validation_x, compression='gzip', compression_opts=9)\n",
    "    hdf.create_dataset('y', data=validation_y, compression='gzip', compression_opts=9)\n",
    "with h5py.File('data\\\\test_unit_13_30.h5', 'w') as hdf:\n",
    "    hdf.create_dataset('x', data=test_unit_13_x, compression='gzip', compression_opts=9)\n",
    "    hdf.create_dataset('y', data=test_unit_13_y, compression='gzip', compression_opts=9)\n",
    "with h5py.File('data\\\\test_unit_14_30.h5', 'w') as hdf:\n",
    "    hdf.create_dataset('x', data=test_unit_14_x, compression='gzip', compression_opts=9)\n",
    "    hdf.create_dataset('y', data=test_unit_14_y, compression='gzip', compression_opts=9)\n",
    "with h5py.File('data\\\\test_unit_15_30.h5', 'w') as hdf:\n",
    "    hdf.create_dataset('x', data=test_unit_15_x, compression='gzip', compression_opts=9)\n",
    "    hdf.create_dataset('y', data=test_unit_15_y, compression='gzip', compression_opts=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
